{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdeac648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching SVB stock data: Jan 2021 - March 10, 2023\n",
      "\n",
      "Fetching 2021...\n",
      "  ✓ 5056 bars\n",
      "Fetching 2022...\n",
      "  ✓ 4711 bars\n",
      "Fetching Jan-March 10, 2023...\n",
      "  ✓ 1391 bars\n",
      "\n",
      "✓ SAVED: svb_stock_jan2021_mar2023.csv\n",
      "  Total bars: 11158\n",
      "  Date range: 2021-01-04 09:30:00 to 2023-03-10 13:30:00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = \"8mcrnuLv_pAWcwLUPdRpGDAg1gHYqlOh\"\n",
    "\n",
    "print(\"Fetching SVB stock data: Jan 2021 - March 10, 2023\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 2021\n",
    "print(\"\\nFetching 2021...\")\n",
    "url = f\"https://api.polygon.io/v2/aggs/ticker/SIVB/range/15/minute/2021-01-01/2021-12-31?adjusted=true&sort=asc&limit=50000&apiKey={API_KEY}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "if 'results' in data:\n",
    "    all_data.extend(data['results'])\n",
    "    print(f\"  ✓ {len(data['results'])} bars\")\n",
    "\n",
    "# 2022\n",
    "print(\"Fetching 2022...\")\n",
    "url = f\"https://api.polygon.io/v2/aggs/ticker/SIVB/range/15/minute/2022-01-01/2022-12-31?adjusted=true&sort=asc&limit=50000&apiKey={API_KEY}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "if 'results' in data:\n",
    "    all_data.extend(data['results'])\n",
    "    print(f\"  ✓ {len(data['results'])} bars\")\n",
    "\n",
    "# 2023 (Jan 1 - March 10)\n",
    "print(\"Fetching Jan-March 10, 2023...\")\n",
    "url = f\"https://api.polygon.io/v2/aggs/ticker/SIVB/range/15/minute/2023-01-01/2023-03-10?adjusted=true&sort=asc&limit=50000&apiKey={API_KEY}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "if 'results' in data:\n",
    "    all_data.extend(data['results'])\n",
    "    print(f\"  ✓ {len(data['results'])} bars\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "df['timestamp'] = pd.to_datetime(df['t'], unit='ms')\n",
    "df = df.rename(columns={'o': 'Open', 'h': 'High', 'l': 'Low', 'c': 'Close', 'v': 'Volume'})\n",
    "df = df[['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "df = df.sort_values('timestamp').drop_duplicates()\n",
    "\n",
    "df.to_csv('svb_stock_jan2021_mar2023.csv', index=False)\n",
    "\n",
    "print(f\"\\n✓ SAVED: svb_stock_jan2021_mar2023.csv\")\n",
    "print(f\"  Total bars: {len(df)}\")\n",
    "print(f\"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b613d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Reddit data: Feb 10 - March 10, 2023 (4 weeks)\n",
      "  Searching r/wallstreetbets for 'SVB'...\n",
      "    Found 89 posts\n",
      "  Searching r/wallstreetbets for 'Silicon Valley Bank'...\n",
      "    Found 36 posts\n",
      "  Searching r/wallstreetbets for 'SIVB'...\n",
      "    Found 51 posts\n",
      "  Searching r/wallstreetbets for 'bank run'...\n",
      "    Found 21 posts\n",
      "  Searching r/stocks for 'SVB'...\n",
      "    Found 14 posts\n",
      "  Searching r/stocks for 'Silicon Valley Bank'...\n",
      "    Found 13 posts\n",
      "  Searching r/stocks for 'SIVB'...\n",
      "    Found 6 posts\n",
      "  Searching r/stocks for 'bank run'...\n",
      "    Found 10 posts\n",
      "  Searching r/investing for 'SVB'...\n",
      "    Found 14 posts\n",
      "  Searching r/investing for 'Silicon Valley Bank'...\n",
      "    Found 9 posts\n",
      "  Searching r/investing for 'SIVB'...\n",
      "    Found 8 posts\n",
      "  Searching r/investing for 'bank run'...\n",
      "    Found 9 posts\n",
      "  Searching r/technology for 'SVB'...\n",
      "    Found 1 posts\n",
      "  Searching r/technology for 'Silicon Valley Bank'...\n",
      "    Found 6 posts\n",
      "  Searching r/technology for 'SIVB'...\n",
      "    Found 0 posts\n",
      "  Searching r/technology for 'bank run'...\n",
      "    Found 0 posts\n",
      "  Searching r/news for 'SVB'...\n",
      "    Found 2 posts\n",
      "  Searching r/news for 'Silicon Valley Bank'...\n",
      "    Found 20 posts\n",
      "  Searching r/news for 'SIVB'...\n",
      "    Found 0 posts\n",
      "  Searching r/news for 'bank run'...\n",
      "    Found 0 posts\n",
      "  Searching r/bayarea for 'SVB'...\n",
      "    Found 4 posts\n",
      "  Searching r/bayarea for 'Silicon Valley Bank'...\n",
      "    Found 13 posts\n",
      "  Searching r/bayarea for 'SIVB'...\n",
      "    Found 0 posts\n",
      "  Searching r/bayarea for 'bank run'...\n",
      "    Found 1 posts\n",
      "  Searching r/finance for 'SVB'...\n",
      "    Found 7 posts\n",
      "  Searching r/finance for 'Silicon Valley Bank'...\n",
      "    Found 9 posts\n",
      "  Searching r/finance for 'SIVB'...\n",
      "    Found 1 posts\n",
      "  Searching r/finance for 'bank run'...\n",
      "    Found 2 posts\n",
      "  Searching r/economics for 'SVB'...\n",
      "    Found 4 posts\n",
      "  Searching r/economics for 'Silicon Valley Bank'...\n",
      "    Found 7 posts\n",
      "  Searching r/economics for 'SIVB'...\n",
      "    Found 1 posts\n",
      "  Searching r/economics for 'bank run'...\n",
      "    Found 1 posts\n",
      "\n",
      "✓ Total unique posts: 325\n",
      "\n",
      "Cleaning posts data...\n",
      "  Removed 156 dirty posts\n",
      "  Clean posts: 169\n",
      "✓ SAVED: svb_reddit_posts_4weeks.csv (169 posts)\n",
      "  Date range: 2023-02-10 15:23:11 to 2023-03-10 23:56:35\n",
      "\n",
      "Fetching comments from top 30 posts...\n",
      "  Post 11nw54y: 100 comments\n",
      "  Post 11nnhzg: 100 comments\n",
      "  Post 11o49kk: 100 comments\n",
      "  Post 11nvd4e: 100 comments\n",
      "  Post 11o4plm: 100 comments\n",
      "  Post 11nue1b: 100 comments\n",
      "  Post 11nzsfc: 100 comments\n",
      "  Post 11ny41q: 100 comments\n",
      "  Post 11nx7eq: 100 comments\n",
      "  Post 10z3ovm: 100 comments\n",
      "  Post 11nuooj: 100 comments\n",
      "  Post 11o0tg3: 100 comments\n",
      "  Post 11nm65s: 73 comments\n",
      "  Post 11nxkje: 100 comments\n",
      "  Post 11nu4pb: 100 comments\n",
      "  Post 11o43hh: 100 comments\n",
      "  Post 11l1ui8: 100 comments\n",
      "  Post 11n5f70: 100 comments\n",
      "  Post 11nuku9: 100 comments\n",
      "  Post 11asa0d: 100 comments\n",
      "  Post 11nqfsm: 100 comments\n",
      "  Post 11nvnq1: 100 comments\n",
      "  Post 11o47qi: 100 comments\n",
      "  Post 11nrlmw: 100 comments\n",
      "  Post 11nzxhs: 35 comments\n",
      "  Post 11nb9f4: 100 comments\n",
      "  Post 11nadgx: 33 comments\n",
      "  Post 11fua92: 100 comments\n",
      "  Post 11myayr: 100 comments\n",
      "  Post 11o2whf: 25 comments\n",
      "\n",
      "Cleaning comments data...\n",
      "  Removed 520 dirty comments\n",
      "  Clean comments: 2246\n",
      "✓ SAVED: svb_reddit_comments_4weeks.csv (2246 comments)\n",
      "\n",
      "============================================================\n",
      "DATA COLLECTION COMPLETE\n",
      "============================================================\n",
      "Clean posts: 169\n",
      "Clean comments: 2246\n",
      "Total texts for GoEmotions: 2415\n",
      "\n",
      "✓ READY FOR GOEMOTIONS IN COLAB\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "SUBREDDITS = [\n",
    "    'wallstreetbets', 'stocks', 'investing', 'technology',\n",
    "    'news', 'bayarea', 'finance', 'economics'\n",
    "]\n",
    "\n",
    "SEARCH_TERMS = ['SVB', 'Silicon Valley Bank', 'SIVB', 'bank run']\n",
    "\n",
    "# Feb 10 - March 10, 2023 (4 weeks)\n",
    "AFTER = int(datetime(2023, 2, 10, 0, 0).timestamp())\n",
    "BEFORE = int(datetime(2023, 3, 10, 23, 59).timestamp())\n",
    "\n",
    "print(\"Fetching Reddit data: Feb 10 - March 10, 2023 (4 weeks)\")\n",
    "\n",
    "all_posts = []\n",
    "\n",
    "for subreddit in SUBREDDITS:\n",
    "    for term in SEARCH_TERMS:\n",
    "        print(f\"  Searching r/{subreddit} for '{term}'...\")\n",
    "        \n",
    "        url = \"https://api.pullpush.io/reddit/search/submission\"\n",
    "        params = {\n",
    "            'subreddit': subreddit,\n",
    "            'q': term,\n",
    "            'after': AFTER,\n",
    "            'before': BEFORE,\n",
    "            'size': 100\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            data = response.json()\n",
    "            posts = data.get('data', [])\n",
    "            all_posts.extend(posts)\n",
    "            print(f\"    Found {len(posts)} posts\")\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "\n",
    "# Remove duplicates\n",
    "unique_posts = {post['id']: post for post in all_posts}\n",
    "all_posts = list(unique_posts.values())\n",
    "\n",
    "print(f\"\\n✓ Total unique posts: {len(all_posts)}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "posts_df = pd.DataFrame([\n",
    "    {\n",
    "        'id': post.get('id'),\n",
    "        'timestamp': datetime.fromtimestamp(post.get('created_utc', 0)),\n",
    "        'subreddit': post.get('subreddit'),\n",
    "        'author': post.get('author'),\n",
    "        'title': post.get('title', ''),\n",
    "        'text': post.get('selftext', ''),\n",
    "        'score': post.get('score', 0),\n",
    "        'num_comments': post.get('num_comments', 0),\n",
    "        'full_text': (post.get('title', '') + ' ' + post.get('selftext', '')).strip()\n",
    "    }\n",
    "    for post in all_posts\n",
    "])\n",
    "\n",
    "# ============================================\n",
    "# DATA CLEANING - POSTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nCleaning posts data...\")\n",
    "initial_posts = len(posts_df)\n",
    "\n",
    "# Remove deleted/removed authors\n",
    "posts_df = posts_df[~posts_df['author'].isin(['[deleted]', '[removed]', 'AutoModerator'])]\n",
    "\n",
    "# Remove empty or too-short text (< 10 characters)\n",
    "posts_df = posts_df[posts_df['full_text'].str.len() >= 10]\n",
    "\n",
    "# Remove posts with deleted/removed content\n",
    "posts_df = posts_df[~posts_df['text'].isin(['[deleted]', '[removed]'])]\n",
    "posts_df = posts_df[~posts_df['title'].isin(['[deleted]', '[removed]'])]\n",
    "\n",
    "# Remove duplicates\n",
    "posts_df = posts_df.drop_duplicates(subset=['full_text'])\n",
    "\n",
    "# Remove bots (common bot names)\n",
    "bot_keywords = ['bot', 'Bot', 'BOT', 'AutoMod']\n",
    "posts_df = posts_df[~posts_df['author'].str.contains('|'.join(bot_keywords), na=False)]\n",
    "\n",
    "# Remove null timestamps\n",
    "posts_df = posts_df.dropna(subset=['timestamp'])\n",
    "\n",
    "print(f\"  Removed {initial_posts - len(posts_df)} dirty posts\")\n",
    "print(f\"  Clean posts: {len(posts_df)}\")\n",
    "\n",
    "posts_df = posts_df.sort_values('timestamp')\n",
    "posts_df.to_csv('svb_reddit_posts_4weeks.csv', index=False)\n",
    "\n",
    "print(f\"✓ SAVED: svb_reddit_posts_4weeks.csv ({len(posts_df)} posts)\")\n",
    "print(f\"  Date range: {posts_df['timestamp'].min()} to {posts_df['timestamp'].max()}\")\n",
    "\n",
    "# ============================================\n",
    "# GET COMMENTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFetching comments from top 30 posts...\")\n",
    "\n",
    "top_posts = posts_df.nlargest(30, 'score')\n",
    "all_comments = []\n",
    "\n",
    "for idx, post in top_posts.iterrows():\n",
    "    url = \"https://api.pullpush.io/reddit/search/comment\"\n",
    "    params = {'link_id': post['id'], 'size': 100}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        data = response.json()\n",
    "        comments = data.get('data', [])\n",
    "        all_comments.extend(comments)\n",
    "        print(f\"  Post {post['id']}: {len(comments)} comments\")\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "comments_df = pd.DataFrame([\n",
    "    {\n",
    "        'id': c.get('id'),\n",
    "        'post_id': c.get('link_id', '').replace('t3_', ''),\n",
    "        'timestamp': datetime.fromtimestamp(c.get('created_utc', 0)),\n",
    "        'author': c.get('author'),\n",
    "        'text': c.get('body', ''),\n",
    "        'score': c.get('score', 0),\n",
    "        'subreddit': c.get('subreddit')\n",
    "    }\n",
    "    for c in all_comments\n",
    "])\n",
    "\n",
    "# ============================================\n",
    "# DATA CLEANING - COMMENTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nCleaning comments data...\")\n",
    "initial_comments = len(comments_df)\n",
    "\n",
    "# Remove deleted/removed authors\n",
    "comments_df = comments_df[~comments_df['author'].isin(['[deleted]', '[removed]', 'AutoModerator'])]\n",
    "\n",
    "# Remove empty or too-short text (< 5 characters)\n",
    "comments_df = comments_df[comments_df['text'].str.len() >= 5]\n",
    "\n",
    "# Remove deleted/removed content\n",
    "comments_df = comments_df[~comments_df['text'].isin(['[deleted]', '[removed]'])]\n",
    "\n",
    "# Remove duplicates\n",
    "comments_df = comments_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Remove bots\n",
    "comments_df = comments_df[~comments_df['author'].str.contains('|'.join(bot_keywords), na=False)]\n",
    "\n",
    "# Remove null timestamps\n",
    "comments_df = comments_df.dropna(subset=['timestamp'])\n",
    "\n",
    "print(f\"  Removed {initial_comments - len(comments_df)} dirty comments\")\n",
    "print(f\"  Clean comments: {len(comments_df)}\")\n",
    "\n",
    "comments_df = comments_df.sort_values('timestamp')\n",
    "comments_df.to_csv('svb_reddit_comments_4weeks.csv', index=False)\n",
    "\n",
    "print(f\"✓ SAVED: svb_reddit_comments_4weeks.csv ({len(comments_df)} comments)\")\n",
    "\n",
    "# ============================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA COLLECTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Clean posts: {len(posts_df)}\")\n",
    "print(f\"Clean comments: {len(comments_df)}\")\n",
    "print(f\"Total texts for GoEmotions: {len(posts_df) + len(comments_df)}\")\n",
    "print(\"\\n✓ READY FOR GOEMOTIONS IN COLAB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9856e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments: 1973\n",
      "Number of posts: 295\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of comments: {len(comments_df)}\")\n",
    "print(f\"Number of posts: {len(posts_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
